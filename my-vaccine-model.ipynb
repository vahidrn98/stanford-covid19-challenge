{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import random\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
    "# from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer\n",
    "# from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from time import perf_counter\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_json('../train.json', lines=True)\n",
    "test = pd.read_json('../test.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.set_index(\"index\")\n",
    "test=test.set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train[train[\"signal_to_noise\"]>=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.assign(pair_prob=[np.load('../bpps/'+row['id']+'.npy') for index,row in train.iterrows()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.assign(pair_prob=[np.load('../bpps/'+row['id']+'.npy') for index,row in test.iterrows()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n",
    "bpps_nb_std = 0.08914  \n",
    "error_mean_limit = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
    "\n",
    "rna_dict    = {x:i for i, x in enumerate('ACGU')} #4\n",
    "struct_dict = {x:i for i, x in enumerate('().')}  #3\n",
    "loop_dict   = {x:i for i, x in enumerate('BEHIMSX')}#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_threshold = 0.15\n",
    "pair_prob = train[\"pair_prob\"].values\n",
    "def create_adj(index):\n",
    "    mat = np.array(pair_prob[index])\n",
    "    shape = mat.shape\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            if(i==j):\n",
    "                mat[i][j] = 1\n",
    "                continue\n",
    "            if(mat[i][j]>pair_threshold):\n",
    "                mat[i][j]=1\n",
    "            else:\n",
    "                mat[i][j]=0\n",
    "    return mat\n",
    "\n",
    "test_pair_prob = test[\"pair_prob\"].values\n",
    "def test_create_adj(index):\n",
    "    mat = np.array(test_pair_prob[index])\n",
    "    shape = mat.shape\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            if(i==j):\n",
    "                mat[i][j] = 1\n",
    "                continue\n",
    "            if(mat[i][j]>pair_threshold):\n",
    "                mat[i][j]=1\n",
    "            else:\n",
    "                mat[i][j]=0\n",
    "    return mat\n",
    "def calc_error_mean(row):\n",
    "    reactivity_error = row['reactivity_error']\n",
    "    deg_error_Mg_pH10 = row['deg_error_Mg_pH10']\n",
    "    deg_error_Mg_50C = row['deg_error_Mg_50C']\n",
    "\n",
    "    return np.mean(np.abs(reactivity_error) +\n",
    "                   np.abs(deg_error_Mg_pH10) + \\\n",
    "                   np.abs(deg_error_Mg_50C)) / 3\n",
    "\n",
    "def calc_sample_weight(row):\n",
    "\n",
    "    error_mean = calc_error_mean(row)\n",
    "    if error_mean >= error_mean_limit:\n",
    "        return 0.\n",
    "\n",
    "    return 1. - error_mean / error_mean_limit\n",
    "def weighted_mse_loss(prds, tgts, weight):\n",
    "    return torch.mean(weight * (prds - tgts)**2)\n",
    "\n",
    "def criterion(prds, tgts):\n",
    "#     print(prds)\n",
    "#     print(tgts)\n",
    "    return (torch.sqrt(torch.nn.MSELoss()(prds, tgts)))\n",
    "\n",
    "def add_edges(edge_index, edge_features, node1, node2, feature1, feature2):\n",
    "    edge_index.append([node1, node2])\n",
    "    edge_features.append(feature1)\n",
    "    edge_index.append([node2, node1])\n",
    "    edge_features.append(feature2)\n",
    "\n",
    "def add_edges_between_base_nodes(edge_index, edge_features, node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        1, # forward edge: 1, backward edge: -1\n",
    "        1, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        -1, # forward edge: 1, backward edge: -1\n",
    "        1, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_paired_nodes(edge_index, edge_features, node1, node2,\n",
    "                                   bpps_value):\n",
    "    edge_feature1 = [\n",
    "        1, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        bpps_value, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        1, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        bpps_value, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_codon_nodes(edge_index, edge_features, node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        1, # is edge between coden nodes\n",
    "        1, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        1, # is edge between coden nodes\n",
    "        -1, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_codon_and_base_node(edge_index, edge_features,\n",
    "                                          node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        1, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        1, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_node(node_features, feature):\n",
    "    node_features.append(feature)\n",
    "\n",
    "def add_base_node(node_features, sequence, predicted_loop_type,\n",
    "                  bpps_sum, bpps_nb):\n",
    "    feature = [\n",
    "        0, # is codon node\n",
    "        sequence == 'A',\n",
    "        sequence == 'C',\n",
    "        sequence == 'G',\n",
    "        sequence == 'U',\n",
    "        predicted_loop_type == 'S',\n",
    "        predicted_loop_type == 'M',\n",
    "        predicted_loop_type == 'I',\n",
    "        predicted_loop_type == 'B',\n",
    "        predicted_loop_type == 'H',\n",
    "        predicted_loop_type == 'E',\n",
    "        predicted_loop_type == 'X',\n",
    "        bpps_sum,\n",
    "        bpps_nb,\n",
    "    ]\n",
    "    add_node(node_features, feature)\n",
    "\n",
    "def add_codon_node(node_features):\n",
    "    feature = [\n",
    "        1, # is codon node\n",
    "        0, # sequence == 'A',\n",
    "        0, # sequence == 'C',\n",
    "        0, # sequence == 'G',\n",
    "        0, # sequence == 'U',\n",
    "        0, # predicted_loop_type == 'S',\n",
    "        0, # predicted_loop_type == 'M',\n",
    "        0, # predicted_loop_type == 'I',\n",
    "        0, # predicted_loop_type == 'B',\n",
    "        0, # predicted_loop_type == 'H',\n",
    "        0, # predicted_loop_type == 'E',\n",
    "        0, # predicted_loop_type == 'X',\n",
    "        0, # bpps_sum\n",
    "        0, # bpps_nb\n",
    "    ]\n",
    "    add_node(node_features, feature)\n",
    "\n",
    "    \n",
    "def build_data(df, is_train):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        targets = []\n",
    "        node_features = []\n",
    "        edge_features = []\n",
    "        edge_index = []\n",
    "        train_mask = []\n",
    "        test_mask = []\n",
    "        weights = []\n",
    "\n",
    "        id = df.loc[i, 'id']\n",
    "        path = os.path.join('../bpps', id + '.npy')\n",
    "        bpps = np.load(path)\n",
    "        bpps_sum = bpps.sum(axis=0)\n",
    "        sequence = df.loc[i, 'sequence']\n",
    "        structure = df.loc[i, 'structure']\n",
    "        pair_info = match_pair(structure)\n",
    "        predicted_loop_type = df.loc[i, 'predicted_loop_type']\n",
    "        seq_length = df.loc[i, 'seq_length']\n",
    "        seq_scored = df.loc[i, 'seq_scored']\n",
    "        bpps_nb = (bpps > 0).sum(axis=0) / seq_length\n",
    "        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n",
    "        if is_train:\n",
    "            sample_weight = calc_sample_weight(df.loc[i])\n",
    "\n",
    "            reactivity = df.loc[i, 'reactivity']\n",
    "            deg_Mg_pH10 = df.loc[i, 'deg_Mg_pH10']\n",
    "            deg_Mg_50C = df.loc[i, 'deg_Mg_50C']\n",
    "\n",
    "            for j in range(seq_length):\n",
    "                if j < seq_scored:\n",
    "                    targets.append([\n",
    "                        reactivity[j],\n",
    "                        deg_Mg_pH10[j],\n",
    "                        deg_Mg_50C[j],\n",
    "                        ])\n",
    "                else:\n",
    "                    targets.append([0, 0, 0])\n",
    "\n",
    "        paired_nodes = {}\n",
    "        for j in range(seq_length):\n",
    "            add_base_node(node_features, sequence[j], predicted_loop_type[j],\n",
    "                          bpps_sum[j], bpps_nb[j])\n",
    "\n",
    "            if j + 1 < seq_length: # edge between current node and next node\n",
    "                add_edges_between_base_nodes(edge_index, edge_features,\n",
    "                                             j, j + 1)\n",
    "\n",
    "            if pair_info[j] != -1:\n",
    "                if pair_info[j] not in paired_nodes:\n",
    "                    paired_nodes[pair_info[j]] = [j]\n",
    "                else:\n",
    "                    paired_nodes[pair_info[j]].append(j)\n",
    "\n",
    "            train_mask.append(j < seq_scored)\n",
    "            test_mask.append(True)\n",
    "            if is_train:\n",
    "                weights.append(sample_weight)\n",
    "\n",
    "       \n",
    "        for pair in paired_nodes.values():\n",
    "            bpps_value = bpps[pair[0], pair[1]]\n",
    "            add_edges_between_paired_nodes(edge_index, edge_features,\n",
    "                                           pair[0], pair[1], bpps_value)\n",
    "\n",
    "       \n",
    "        codon_node_idx = seq_length - 1\n",
    "        for j in range(seq_length):\n",
    "            if j % 3 == 0:\n",
    "                # add codon node\n",
    "                add_codon_node(node_features)\n",
    "                codon_node_idx += 1\n",
    "                train_mask.append(False)\n",
    "                test_mask.append(False)\n",
    "                if is_train:\n",
    "                    weights.append(0)\n",
    "                    targets.append([0, 0, 0])\n",
    "\n",
    "                if codon_node_idx > seq_length:\n",
    "                    # add edges between adjacent codon nodes\n",
    "                    add_edges_between_codon_nodes(edge_index, edge_features,\n",
    "                                                  codon_node_idx - 1,\n",
    "                                                  codon_node_idx)\n",
    "\n",
    "            # add edges between codon node and base node\n",
    "            add_edges_between_codon_and_base_node(edge_index, edge_features,\n",
    "                                                  j, codon_node_idx)\n",
    "\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "        if is_train:\n",
    "            data.append(MyData(x=node_features, edge_index=edge_index,\n",
    "                               edge_attr=edge_features,\n",
    "                               train_mask=torch.tensor(train_mask),\n",
    "                               weight=torch.tensor(weights, dtype=torch.float),\n",
    "                               y=torch.tensor(targets, dtype=torch.float)))\n",
    "        else:\n",
    "            data.append(MyData(x=node_features, edge_index=edge_index,\n",
    "                               edge_attr=edge_features,\n",
    "                               test_mask=torch.tensor(test_mask)))\n",
    "\n",
    "    return data\n",
    "\n",
    "def np_onehot(x, max=54):\n",
    "    return np.eye(max)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        y = x * torch.sigmoid(x)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        return grad_output * (sigmoid * (1 + x * (1 - sigmoid)))\n",
    "F_swish = SwishFunction.apply\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F_swish(x)\n",
    "class ConvBn1d(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size, padding=0, dilation=1):\n",
    "        super(ConvBn1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F_swish(x)\n",
    "        x = F.dropout(x,0.2,training=self.training)\n",
    "        return x\n",
    "class PositionEncode(nn.Module):\n",
    "    def __init__(self, dim, length=130):\n",
    "        super(PositionEncode, self).__init__()\n",
    "        position = torch.zeros(length,dim)\n",
    "        p = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        position[:,0::2] = torch.sin(p * div)\n",
    "        position[:,1::2] = torch.cos(p * div)\n",
    "        position = position.transpose(0, 1).reshape(1,dim,length) #.contiguous()\n",
    "        self.register_buffer('position', position)\n",
    "\n",
    "        #self.position = nn.Parameter( torch.randn(1, dim, length) ) #random\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, length, dim = x.shape\n",
    "\n",
    "        position = self.position.repeat(batch_size, 1, 1)\n",
    "        position = position[:, :, :length].contiguous()\n",
    "        return position\n",
    "\n",
    "# d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        num_target=5\n",
    "        self.position = PositionEncode(64)\n",
    "        self.cnn = nn.ModuleList([\n",
    "            #ConvBn1d( 14,  32, kernel_size=1,  padding=0),\n",
    "            ConvBn1d( 16,  64, kernel_size=5,  padding=2),\n",
    "            ConvBn1d( 16, 128, kernel_size=11, padding=5),\n",
    "            ConvBn1d( 16, 128, kernel_size=21, padding=10),\n",
    "        ])\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(384, 64, 1024, dropout=0.1, activation='relu'),\n",
    "            2\n",
    "        )\n",
    "        # self.rnn = nn.GRU(256, 256, num_layers=2, batch_first=True, dropout=0, bidirectional=True)\n",
    "        self.predict = nn.Linear(384,num_target)\n",
    "\n",
    "    #https://discuss.pytorch.org/t/clarification-regarding-the-return-of-nn-gru/47363/2\n",
    "    def forward(self, sequence):\n",
    "        batch_size, length, dim = sequence.shape\n",
    "\n",
    "        pos  = self.position(sequence)\n",
    "        sequence = sequence.permute(0,2,1).contiguous()\n",
    "        seq = [cnn(sequence) for cnn in self.cnn]\n",
    "        x = torch.cat(seq+[pos], 1)\n",
    "        x = x.permute(-1, 0, 1) #torch.Size([107, 8, 512])\n",
    "        x = F.dropout(x,0.1,training=self.training)\n",
    "\n",
    "        x = self.transformer(x) #torch.Size([107, 8, 512])\n",
    "        x = x.permute(1, 0, 2).contiguous() #torch.Size([8, 512, 107])\n",
    "\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        predict = self.predict(x)\n",
    "        return predict\n",
    "\n",
    "\n",
    "\n",
    "def mse_loss(predict,target):\n",
    "    batch_size,length, num_target = target.shape\n",
    "    predict = predict[:,:length]\n",
    "    loss = F.mse_loss(predict,target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/c/stanford-covid-vaccine/discussion/183211\n",
    "def mcrmse_loss(predict,target):\n",
    "    batch_size,length, num_target = target.shape\n",
    "    predict = predict[:,:length]\n",
    "    predict = predict.reshape(-1,num_target)\n",
    "    target  = target.reshape(-1,num_target)\n",
    "\n",
    "    l = (predict-target)**2\n",
    "    l = l.mean(0)\n",
    "    l = l**0.5\n",
    "    loss = l.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D(nn.Module):\n",
    "    def __init__(self,layer_size,channel,drop1,drop2):\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((34, 2))\n",
    "        self.conv1 = nn.Conv2d(2, channel, (3,2))\n",
    "        self.conv2 = nn.Conv2d(channel, channel, (2,1))\n",
    "        self.fc1 = nn.Linear(channel * 31, layer_size)\n",
    "        self.fc2 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc3 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc4 = nn.Linear(layer_size, 68)\n",
    "        self.drop1 = drop1\n",
    "        self.drop2 = drop2\n",
    "\n",
    "    def forward(self, x,channel):\n",
    "#         x = self.avgpool(x)\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = x.view(-1, channel * 31)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.dropout(x,self.drop1)\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.dropout(x,self.drop2)\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed = 777\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=train['sequence'].map(lambda seq: [rna_dict[x] for x in seq]).tolist()\n",
    "struct = train['structure'].map(lambda seq: [struct_dict[x] for x in seq]).tolist()\n",
    "loop   = train['predicted_loop_type'].map(lambda seq: [loop_dict[x] for x in seq]).tolist()\n",
    "test_sequence=test['sequence'].map(lambda seq: [rna_dict[x] for x in seq]).tolist()\n",
    "test_struct = test['structure'].map(lambda seq: [struct_dict[x] for x in seq]).tolist()\n",
    "test_loop   = test['predicted_loop_type'].map(lambda seq: [loop_dict[x] for x in seq]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.array(sequence))\n",
    "sequence=scaler.transform(np.array(sequence))\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.array(loop))\n",
    "loop=scaler.transform(np.array(loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(arr):\n",
    "    for x in range(len(arr)):\n",
    "        maxi = max(arr[x])\n",
    "        mini = min(arr[x])\n",
    "        for y in  range(len(arr[x])):\n",
    "            arr[x][y]=(arr[x][y]-mini)/(maxi-mini)\n",
    "            \n",
    "minmax(test_sequence)\n",
    "minmax(test_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_item(index):\n",
    "    data = []\n",
    "    data.append(np.array(np_onehot(sequence[index],4)))\n",
    "    data.append(np_onehot(loop[index],7))\n",
    "    data.append(create_adj(index))\n",
    "    return (data)\n",
    "\n",
    "def get_test_item(index):\n",
    "    data = []\n",
    "    data.append(np.array(np_onehot(test_sequence[index],4)))\n",
    "    data.append(np_onehot(test_loop[index],7))\n",
    "    data.append(test_create_adj(index))\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folded_seq(index):\n",
    "    final = []\n",
    "    seq = sequence[index]\n",
    "    if(len(seq)==107):\n",
    "        seq = seq[0:68]\n",
    "        final.append(seq[0:34])\n",
    "        final.append(list(reversed(seq[34:68])))\n",
    "    else:\n",
    "        seq = seq[0:92]\n",
    "        final.append(seq[0:46])\n",
    "        final.append(list(reversed(seq[46:92])))\n",
    "    return final\n",
    "\n",
    "def get_folded_loop(index):\n",
    "    final = []\n",
    "    seq = loop[index]\n",
    "    if(len(seq)==107):\n",
    "        seq = seq[0:68]\n",
    "        final.append(seq[0:34])\n",
    "        final.append(list(reversed(seq[34:68])))\n",
    "    else:\n",
    "        seq = seq[0:92]\n",
    "        final.append(seq[0:46])\n",
    "        final.append(list(reversed(seq[46:92])))\n",
    "    return final\n",
    "\n",
    "def test_get_folded_seq(index):\n",
    "    final = []\n",
    "    seq = test_sequence[index]\n",
    "    if(len(seq)==107):\n",
    "        seq = seq[0:68]\n",
    "        final.append(seq[0:34])\n",
    "        final.append(list(reversed(seq[34:68])))\n",
    "    else:\n",
    "        seq = seq[0:92]\n",
    "        final.append(seq[0:46])\n",
    "        final.append(list(reversed(seq[46:92])))\n",
    "    return final\n",
    "\n",
    "def test_get_folded_loop(index):\n",
    "    final = []\n",
    "    seq = test_loop[index]\n",
    "    if(len(seq)==107):\n",
    "        seq = seq[0:68]\n",
    "        final.append(seq[0:34])\n",
    "        final.append(list(reversed(seq[34:68])))\n",
    "    else:\n",
    "        seq = seq[0:92]\n",
    "        final.append(seq[0:46])\n",
    "        final.append(list(reversed(seq[46:92])))\n",
    "    return final\n",
    "\n",
    "def build_channel(index):    \n",
    "    final = []\n",
    "    for x,y in zip(get_folded_seq(index),get_folded_loop(index)):\n",
    "        tmp = []\n",
    "        for a,b in zip(x,y):\n",
    "            tmp.append([a,b])\n",
    "        final.append(tmp)\n",
    "    return final\n",
    "\n",
    "def test_build_channel(index):    \n",
    "    final = []\n",
    "    for x,y in zip(test_get_folded_seq(index),test_get_folded_loop(index)):\n",
    "        tmp = []\n",
    "        for a,b in zip(x,y):\n",
    "            tmp.append([a,b])\n",
    "        final.append(tmp)\n",
    "    return final\n",
    "\n",
    "def get_one_hot(index,base,adj):\n",
    "#     start = perf_counter()\n",
    "#     end = perf_counter()\n",
    "#     print(end-start)\n",
    "#     print(\"num of paired: \",sum(adj[base]))\n",
    "    seq = np.zeros((4,5))\n",
    "    loops = np.zeros((4,8))\n",
    "    j = 0\n",
    "#     print(train.iloc[index][\"pair_prob\"][base])\n",
    "    x = adj[base]\n",
    "    for i in range(len(x)):\n",
    "        if(j==3):\n",
    "            break\n",
    "        if(x[i]==1):\n",
    "#             print(np_onehot(sequence[index][base]+1,5))\n",
    "            seq[j]=(np_onehot(sequence[index][base]+1,5))\n",
    "            loops[j]=(np_onehot(loop[index][base]+1,8))\n",
    "            j+=1\n",
    "#     print(j)\n",
    "    return (seq,loops)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   0,    1,    2, ..., 2094, 2095, 2096]), array([420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
      "       433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
      "       446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
      "       459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471,\n",
      "       472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484,\n",
      "       485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497,\n",
      "       498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510,\n",
      "       511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523,\n",
      "       524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536,\n",
      "       537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549,\n",
      "       550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,\n",
      "       563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575,\n",
      "       576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588,\n",
      "       589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
      "       602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614,\n",
      "       615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627,\n",
      "       628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640,\n",
      "       641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653,\n",
      "       654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666,\n",
      "       667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679,\n",
      "       680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692,\n",
      "       693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705,\n",
      "       706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718,\n",
      "       719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731,\n",
      "       732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744,\n",
      "       745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757,\n",
      "       758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770,\n",
      "       771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783,\n",
      "       784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796,\n",
      "       797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809,\n",
      "       810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822,\n",
      "       823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835,\n",
      "       836, 837, 838, 839]))\n"
     ]
    }
   ],
   "source": [
    "xs = train\n",
    "ys = train[['reactivity','deg_Mg_pH10','deg_Mg_50C']]\n",
    "x_train,x_test,y_train,y_test = train_test_split(xs,ys,test_size=0.2,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(layer_size,alpha,batch_size,channel,epochs,drop1,drop2):\n",
    "    net = CNN2D(layer_size,channel,drop1,drop2)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=alpha)\n",
    "    losses = []\n",
    "    avgpool = nn.AdaptiveAvgPool2d((34, 2))\n",
    "    for epoch in range(epochs):\n",
    "        batch = []\n",
    "        cnn_input = []\n",
    "        results = []\n",
    "        ys = []\n",
    "        net.train()\n",
    "        for x in range(1500):\n",
    "\n",
    "            input_t=build_channel(x)\n",
    "            batch.append(input_t)\n",
    "            ys.append(train.iloc[x][\"reactivity\"])\n",
    "            if(len(batch)%batch_size==batch_size-1):\n",
    "#                 print(torch.Tensor(batch).shape)\n",
    "                batch_input = avgpool(torch.Tensor(batch).to(device))\n",
    "                result = net(batch_input,channel)\n",
    "                loss = criterion(result, torch.Tensor(ys).to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                batch = []\n",
    "                ys = []\n",
    "    return sum(losses)/len(losses),net\n",
    "\n",
    "def test_cnn(cnn,batch_size,channel,epochs):\n",
    "    net = cnn\n",
    "#     optimizer = torch.optim.Adam(net.parameters(), lr=alpha)\n",
    "    losses = []\n",
    "    avgpool = nn.AdaptiveAvgPool2d((34, 2))\n",
    "    for epoch in range(epochs):\n",
    "        batch = []\n",
    "        cnn_input = []\n",
    "        results = []\n",
    "        ys = []\n",
    "        net.eval()\n",
    "        for x in range(1500,len(train)):\n",
    "\n",
    "            input_t=build_channel(x)\n",
    "            batch.append(input_t)\n",
    "            ys.append(train.iloc[x][\"reactivity\"])\n",
    "            if(len(batch)%batch_size==batch_size-1):\n",
    "#                 print(torch.Tensor(batch).shape)\n",
    "                batch_input = avgpool(torch.Tensor(batch).to(device))\n",
    "                result = net(batch_input,channel)\n",
    "                loss = criterion(result, torch.Tensor(ys).to(device))\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "#                 optimizer.zero_grad()\n",
    "                batch = []\n",
    "                ys = []\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-02 22:06:27,374] A new study created in memory with name: no-name-d9b2848f-d7e2-439b-9220-149cd3a4d352\n",
      "[I 2020-10-02 22:07:58,723] Trial 0 finished with value: 0.36479505045073374 and parameters: {'test_batch_size': 82, 'epochs': 76, 'dropout_rate': 0.6266424260925496, 'dropout_rate2': 0.1925256072483852}. Best is trial 0 with value: 0.36479505045073374.\n",
      "[I 2020-10-02 22:08:47,990] Trial 1 finished with value: 0.37408627657329335 and parameters: {'test_batch_size': 36, 'epochs': 41, 'dropout_rate': 0.32559093668713524, 'dropout_rate2': 0.7838413687762356}. Best is trial 0 with value: 0.36479505045073374.\n",
      "[I 2020-10-02 22:09:47,282] Trial 2 finished with value: 0.3656082192192907 and parameters: {'test_batch_size': 26, 'epochs': 46, 'dropout_rate': 0.7015473193011258, 'dropout_rate2': 0.013918710380957156}. Best is trial 0 with value: 0.36479505045073374.\n",
      "[I 2020-10-02 22:10:23,195] Trial 3 finished with value: 0.3552745034297307 and parameters: {'test_batch_size': 96, 'epochs': 30, 'dropout_rate': 0.4151692290825866, 'dropout_rate2': 0.07682601860912042}. Best is trial 3 with value: 0.3552745034297307.\n",
      "[I 2020-10-02 22:12:06,976] Trial 4 finished with value: 0.35824116538552675 and parameters: {'test_batch_size': 35, 'epochs': 88, 'dropout_rate': 0.10603585844829544, 'dropout_rate2': 0.5305772061883702}. Best is trial 3 with value: 0.3552745034297307.\n",
      "[I 2020-10-02 22:13:43,682] Trial 5 finished with value: 0.37635918187372613 and parameters: {'test_batch_size': 10, 'epochs': 80, 'dropout_rate': 0.8448695458152431, 'dropout_rate2': 0.4068911552317006}. Best is trial 3 with value: 0.3552745034297307.\n",
      "[I 2020-10-02 22:15:17,197] Trial 6 finished with value: 0.3472552731633186 and parameters: {'test_batch_size': 30, 'epochs': 82, 'dropout_rate': 0.15673583351412124, 'dropout_rate2': 0.31710456335242154}. Best is trial 6 with value: 0.3472552731633186.\n",
      "[I 2020-10-02 22:17:03,132] Trial 7 finished with value: 0.3752771019935608 and parameters: {'test_batch_size': 29, 'epochs': 82, 'dropout_rate': 0.3880869658399607, 'dropout_rate2': 0.7961910172150218}. Best is trial 6 with value: 0.3472552731633186.\n",
      "[I 2020-10-02 22:18:00,285] Trial 8 finished with value: 0.37084485292434693 and parameters: {'test_batch_size': 60, 'epochs': 40, 'dropout_rate': 0.3770924704137588, 'dropout_rate2': 0.6641446283621887}. Best is trial 6 with value: 0.3472552731633186.\n",
      "[I 2020-10-02 22:18:14,338] Trial 9 finished with value: 0.3678518533706665 and parameters: {'test_batch_size': 75, 'epochs': 10, 'dropout_rate': 0.11495003951045846, 'dropout_rate2': 0.5406802182482573}. Best is trial 6 with value: 0.3472552731633186.\n",
      "[I 2020-10-02 22:19:40,701] Trial 10 finished with value: 0.34293014252627335 and parameters: {'test_batch_size': 12, 'epochs': 65, 'dropout_rate': 0.002168278251069078, 'dropout_rate2': 0.2967157757775833}. Best is trial 10 with value: 0.34293014252627335.\n",
      "[I 2020-10-02 22:20:56,946] Trial 11 finished with value: 0.3453964293003082 and parameters: {'test_batch_size': 10, 'epochs': 64, 'dropout_rate': 0.015353746546395004, 'dropout_rate2': 0.2898174660474585}. Best is trial 10 with value: 0.34293014252627335.\n",
      "[I 2020-10-02 22:22:18,159] Trial 12 finished with value: 0.34162370077634263 and parameters: {'test_batch_size': 11, 'epochs': 63, 'dropout_rate': 0.036328120467448155, 'dropout_rate2': 0.24898128528184402}. Best is trial 12 with value: 0.34162370077634263.\n",
      "[I 2020-10-02 22:23:32,659] Trial 13 finished with value: 0.33178016056819837 and parameters: {'test_batch_size': 13, 'epochs': 63, 'dropout_rate': 0.004498827687312539, 'dropout_rate2': 0.16198688217508977}. Best is trial 13 with value: 0.33178016056819837.\n",
      "[I 2020-10-02 22:25:28,214] Trial 14 finished with value: 0.35094986053613514 and parameters: {'test_batch_size': 45, 'epochs': 97, 'dropout_rate': 0.23942110731584582, 'dropout_rate2': 0.12386638968763114}. Best is trial 13 with value: 0.33178016056819837.\n",
      "[I 2020-10-02 22:26:38,457] Trial 15 finished with value: 0.33007446442331584 and parameters: {'test_batch_size': 18, 'epochs': 60, 'dropout_rate': 0.011350540506216382, 'dropout_rate2': 0.18320257230876474}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:27:41,783] Trial 16 finished with value: 0.40040269109510607 and parameters: {'test_batch_size': 20, 'epochs': 52, 'dropout_rate': 0.23815465443021858, 'dropout_rate2': 0.9913577686610813}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:28:11,986] Trial 17 finished with value: 0.36157549917697906 and parameters: {'test_batch_size': 50, 'epochs': 26, 'dropout_rate': 0.5906891087675521, 'dropout_rate2': 0.06712588752808103}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:29:40,661] Trial 18 finished with value: 0.34793459888427486 and parameters: {'test_batch_size': 20, 'epochs': 70, 'dropout_rate': 0.22219837435202155, 'dropout_rate2': 0.41359094245868533}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:30:52,148] Trial 19 finished with value: 0.40102308094501493 and parameters: {'test_batch_size': 58, 'epochs': 56, 'dropout_rate': 0.9932829304070052, 'dropout_rate2': 0.15571182369412126}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:31:28,265] Trial 20 finished with value: 0.3393434972474069 and parameters: {'test_batch_size': 19, 'epochs': 29, 'dropout_rate': 0.04963705649319106, 'dropout_rate2': 0.010346316227524521}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:31:41,795] Trial 21 finished with value: 0.3477999971758935 and parameters: {'test_batch_size': 20, 'epochs': 10, 'dropout_rate': 0.013741943555499722, 'dropout_rate2': 0.012413967061756046}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:32:09,126] Trial 22 finished with value: 0.3494576930999756 and parameters: {'test_batch_size': 40, 'epochs': 23, 'dropout_rate': 0.06827073570483619, 'dropout_rate2': 0.18005605754407805}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:33:20,212] Trial 23 finished with value: 0.3522067160317392 and parameters: {'test_batch_size': 19, 'epochs': 55, 'dropout_rate': 0.161345944631439, 'dropout_rate2': 0.029756147000648714}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:34:14,853] Trial 24 finished with value: 0.35215148571375254 and parameters: {'test_batch_size': 17, 'epochs': 33, 'dropout_rate': 0.006041921809346623, 'dropout_rate2': 0.42093899563076054}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:34:37,313] Trial 25 finished with value: 0.34900590777397156 and parameters: {'test_batch_size': 27, 'epochs': 18, 'dropout_rate': 0.186311253373325, 'dropout_rate2': 0.000431530200867003}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:35:37,821] Trial 26 finished with value: 0.3476800322532654 and parameters: {'test_batch_size': 67, 'epochs': 49, 'dropout_rate': 0.28684669841011623, 'dropout_rate2': 0.11700108921254998}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:37:21,885] Trial 27 finished with value: 0.3493127112205212 and parameters: {'test_batch_size': 44, 'epochs': 72, 'dropout_rate': 0.09247112897687848, 'dropout_rate2': 0.257171744763453}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:38:08,761] Trial 28 finished with value: 0.35958369904094273 and parameters: {'test_batch_size': 14, 'epochs': 39, 'dropout_rate': 0.5078002444971698, 'dropout_rate2': 0.207309101603154}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:39:24,750] Trial 29 finished with value: 0.3410368512074153 and parameters: {'test_batch_size': 94, 'epochs': 61, 'dropout_rate': 0.07995313797679432, 'dropout_rate2': 0.3600945922299397}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:40:48,066] Trial 30 finished with value: 0.3642516279220581 and parameters: {'test_batch_size': 24, 'epochs': 72, 'dropout_rate': 0.7446687672975569, 'dropout_rate2': 0.1838538405986105}. Best is trial 15 with value: 0.33007446442331584.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-02 22:41:59,296] Trial 31 finished with value: 0.34233276546001434 and parameters: {'test_batch_size': 98, 'epochs': 62, 'dropout_rate': 0.06822048778273514, 'dropout_rate2': 0.3676579561959485}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:42:58,553] Trial 32 finished with value: 0.3503526863124635 and parameters: {'test_batch_size': 34, 'epochs': 47, 'dropout_rate': 0.14097251427996169, 'dropout_rate2': 0.35122068622752395}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:44:04,016] Trial 33 finished with value: 0.34898888568083447 and parameters: {'test_batch_size': 91, 'epochs': 58, 'dropout_rate': 0.2999884280316211, 'dropout_rate2': 0.4678815695237462}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:45:12,945] Trial 34 finished with value: 0.3493573580469404 and parameters: {'test_batch_size': 84, 'epochs': 60, 'dropout_rate': 0.01281695390132842, 'dropout_rate2': 0.0893311730311346}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:46:30,787] Trial 35 finished with value: 0.3410576991736889 and parameters: {'test_batch_size': 71, 'epochs': 67, 'dropout_rate': 0.06040298184102004, 'dropout_rate2': 0.22974610772328632}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:48:23,365] Trial 36 finished with value: 0.3542072892189026 and parameters: {'test_batch_size': 24, 'epochs': 76, 'dropout_rate': 0.10789513818867706, 'dropout_rate2': 0.0562657616042871}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:49:23,621] Trial 37 finished with value: 0.3491546469075339 and parameters: {'test_batch_size': 84, 'epochs': 43, 'dropout_rate': 0.19924605300937331, 'dropout_rate2': 0.12889224164273383}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:50:12,529] Trial 38 finished with value: 0.3627948368850507 and parameters: {'test_batch_size': 32, 'epochs': 34, 'dropout_rate': 0.13235805437033824, 'dropout_rate2': 0.62681011328073}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:52:20,854] Trial 39 finished with value: 0.3456018030643463 and parameters: {'test_batch_size': 39, 'epochs': 88, 'dropout_rate': 0.3360014717762268, 'dropout_rate2': 0.3199872793433076}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:53:18,435] Trial 40 finished with value: 0.40147961392289117 and parameters: {'test_batch_size': 15, 'epochs': 51, 'dropout_rate': 0.9939637486916338, 'dropout_rate2': 0.4994740495757169}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:54:37,334] Trial 41 finished with value: 0.3468637764453888 and parameters: {'test_batch_size': 73, 'epochs': 68, 'dropout_rate': 0.06634972073770252, 'dropout_rate2': 0.22757885545423095}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:56:05,559] Trial 42 finished with value: 0.33272866408030194 and parameters: {'test_batch_size': 94, 'epochs': 68, 'dropout_rate': 0.03786473209851559, 'dropout_rate2': 0.2656080654693047}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:57:36,076] Trial 43 finished with value: 0.3547118504842122 and parameters: {'test_batch_size': 92, 'epochs': 77, 'dropout_rate': 0.4733749070129605, 'dropout_rate2': 0.27426125928600276}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:58:44,067] Trial 44 finished with value: 0.35198068122069043 and parameters: {'test_batch_size': 91, 'epochs': 60, 'dropout_rate': 0.03442542581765046, 'dropout_rate2': 0.33534223240971694}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 22:59:43,700] Trial 45 finished with value: 0.3445376257101695 and parameters: {'test_batch_size': 100, 'epochs': 53, 'dropout_rate': 0.10689959584871256, 'dropout_rate2': 0.15972671207676287}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 23:01:26,449] Trial 46 finished with value: 0.3457378049691518 and parameters: {'test_batch_size': 95, 'epochs': 86, 'dropout_rate': 0.15523701754366193, 'dropout_rate2': 0.3982106494355604}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 23:02:44,746] Trial 47 finished with value: 0.3511787610394614 and parameters: {'test_batch_size': 80, 'epochs': 66, 'dropout_rate': 0.010921443473412756, 'dropout_rate2': 0.2996839573695274}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 23:04:20,846] Trial 48 finished with value: 0.3463578273852666 and parameters: {'test_batch_size': 88, 'epochs': 75, 'dropout_rate': 0.052290167642695215, 'dropout_rate2': 0.09575466613726609}. Best is trial 15 with value: 0.33007446442331584.\n",
      "[I 2020-10-02 23:05:25,078] Trial 49 finished with value: 0.34794708865660207 and parameters: {'test_batch_size': 12, 'epochs': 43, 'dropout_rate': 0.1847485286177591, 'dropout_rate2': 0.20858680863499107}. Best is trial 15 with value: 0.33007446442331584.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    layer_size = 100\n",
    "    alpha = 0.005\n",
    "    batch_size = 80\n",
    "    test_batch_size = trial.suggest_int('test_batch_size', 10, 100)\n",
    "    epochs = trial.suggest_int('epochs', 10, 100)\n",
    "    drop1 = trial.suggest_uniform('dropout_rate', 0.0, 1.0)\n",
    "    drop2 = trial.suggest_uniform('dropout_rate2', 0.0, 1.0)\n",
    "    channels = 5\n",
    "    results,net = train_cnn(layer_size,alpha,batch_size,channels,epochs,drop1,drop2)\n",
    "    test_results = test_cnn(net,test_batch_size,channels,1)\n",
    "    return test_results\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self,layer_size):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(52, layer_size)\n",
    "        self.fc2 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc3 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc4 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc5 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc6 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc7 = nn.Linear(layer_size,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "#         x = F.dropout(x,self.drop1)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = F.dropout(x,self.drop2)\n",
    "#         x = self.fc3(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = F.dropout(x,self.drop3)\n",
    "#         x = self.fc4(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = F.dropout(x,self.drop3)\n",
    "#         x = self.fc5(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = F.dropout(x,self.drop3)\n",
    "        x = self.fc7(x)\n",
    "        output =  x = torch.relu(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn(layer_size,alpha,batch_size):    \n",
    "    dnn = DNN(layer_size)\n",
    "    epochs = 1\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=alpha)\n",
    "    losses = []\n",
    "    input_t =[]\n",
    "    for epoch in range(epochs):\n",
    "        batch = []\n",
    "        cnn_input = []\n",
    "        results = []\n",
    "        ys = []\n",
    "        dnn.train()\n",
    "        react = train[\"reactivity\"].values\n",
    "        for x in tqdm(range(len(train))):\n",
    "            adj = create_adj(x)\n",
    "            for y in range(train.iloc[x][\"seq_scored\"]):\n",
    "    #             start = perf_counter()\n",
    "                one_hot = get_one_hot(x,y,adj)\n",
    "\n",
    "                for s in (one_hot[0]):\n",
    "    #                 print(s)\n",
    "                    input_t.extend(s)\n",
    "                for l in (one_hot[1]):\n",
    "    #                 print(l)\n",
    "                    input_t.extend(l)\n",
    "    #             end = perf_counter()\n",
    "    #             print(end-start)\n",
    "    #             print(input_t)\n",
    "    #             break\n",
    "    #             start=perf_counter()\n",
    "                optimizer.zero_grad()\n",
    "                batch.append(input_t)\n",
    "                input_t = []\n",
    "                ground = [react[x][y]]\n",
    "                ys.append(ground)\n",
    "    #             end = perf_counter()\n",
    "    #             print(end-start)\n",
    "    #             ys = np.asarray(ys).reshape((1,49))\n",
    "                if(len(batch)%batch_size==batch_size-1):\n",
    "                    result = dnn(torch.Tensor(batch).to(device))\n",
    "                    loss = criterion(result, torch.Tensor(ys).to(device))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    losses.append(loss.item())\n",
    "                    batch = []\n",
    "                    ys = []\n",
    "    return (sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    layer_size = trial.suggest_int('layer_size', 10, 50)\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-6, 1e-3)\n",
    "    batch_size = trial.suggest_int('batch_size', 500, 2000)\n",
    "    results = train_dnn(layer_size,alpha,batch_size)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(losses)/len(losses))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
