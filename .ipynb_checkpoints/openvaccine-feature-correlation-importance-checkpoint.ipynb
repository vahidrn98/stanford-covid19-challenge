{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction Getting Started Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A starter notebook for those planning to build your submission from scratch. Will update with EDA as and when time permits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents how to find correlation between features and grade features by importance for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train and test json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_json('../input/stanford-covid-vaccine/train.json', lines=True)\n",
    "train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so intuitive, right. Lets see if we can flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = [json.loads(line) for line in open('../input/stanford-covid-vaccine/train.json', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_json(input_json):\n",
    "    \n",
    "    for index,json_ in enumerate(input_json):\n",
    "        length = json_['seq_scored']\n",
    "        json_['step'] = list(range(length))\n",
    "            \n",
    "        json_['sequence'] = pd.Series([json_['sequence']]).map(lambda seq: [token2int[x] for x in seq]).values.tolist()[0][:length]\n",
    "        json_['structure'] = pd.Series([json_['structure']]).map(lambda seq: [token2int[x] for x in seq]).values.tolist()[0][:length]\n",
    "        json_['predicted_loop_type'] = pd.Series([json_['predicted_loop_type']]).map(lambda seq: [token2int[x] for x in seq]).values.tolist()[0][:length]\n",
    "        if os.path.exists('../input/stanford-covid-vaccine/bpps/'+json_['id']+'.npy'):\n",
    "            json_['unpaired_probability'] = list(1-sum(np.load('../input/stanford-covid-vaccine/bpps/'+json_['id']+'.npy')))[:length]\n",
    "        else:\n",
    "            print('bpps not found')\n",
    "\n",
    "preprocess_json(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(input_json):\n",
    "    train = pd.json_normalize(data = input_json, \n",
    "                                record_path ='step',  \n",
    "                                meta =['id','seq_length','seq_scored']) \n",
    "    train.rename(columns={0:'step'}, inplace=True)\n",
    "    train['unpaired_probability'] = pd.json_normalize(data = input_json, \n",
    "                                record_path ='unpaired_probability'\n",
    "                                            )\n",
    "    train['sequence'] = pd.json_normalize(data = input_json, \n",
    "                                record_path ='sequence'\n",
    "                                            )\n",
    "    train['structure'] = pd.json_normalize(data = input_json, \n",
    "                                record_path ='structure'\n",
    "                                            )\n",
    "    train['predicted_loop_type'] = pd.json_normalize(data = input_json, \n",
    "                                record_path ='predicted_loop_type'\n",
    "                                            )\n",
    "    train['reactivity'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='reactivity'\n",
    "                                                )\n",
    "    train['reactivity_error'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='reactivity_error'\n",
    "                                                )\n",
    "    train['deg_Mg_pH10'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_Mg_pH10'\n",
    "                                                )\n",
    "    train['deg_error_Mg_pH10'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_error_Mg_pH10'\n",
    "                                                )\n",
    "    train['deg_pH10'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_pH10',\n",
    "                                                )\n",
    "    train['deg_error_pH10'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_error_pH10',\n",
    "                                                )\n",
    "    train['deg_Mg_50C'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_Mg_50C',\n",
    "                                                )\n",
    "    train['deg_error_Mg_50C'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_error_Mg_50C',\n",
    "                                                )\n",
    "    train['deg_50C'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_50C',\n",
    "                                                )\n",
    "    train['deg_error_50C'] = pd.json_normalize(data = input_json, \n",
    "                                    record_path ='deg_error_50C',\n",
    "                                                )\n",
    "        \n",
    "    train.set_index(['id','step'], inplace=True)\n",
    "    return train\n",
    "\n",
    "X_train = process_json(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_json(input_df):\n",
    "    input_df = pd.concat([input_df.drop('sequence', axis=1), pd.get_dummies(input_df['sequence'], prefix='Base')], axis=1)\n",
    "    input_df = pd.concat([input_df.drop('structure', axis=1), pd.get_dummies(input_df['structure'], prefix='Structure')], axis=1)\n",
    "    input_df = pd.concat([input_df.drop('predicted_loop_type', axis=1), pd.get_dummies(input_df['predicted_loop_type'], prefix='Loop')], axis=1)\n",
    "    return input_df\n",
    "\n",
    "X_train_full = post_process_json(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C']\n",
    "y_train = X_train_full[label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['Base_3','Base_4','Base_5','Base_6','Structure_0','Structure_1','Structure_2','Loop_7','Loop_8','Loop_9','Loop_10','Loop_11','Loop_12','Loop_13']\n",
    "X_train = X_train_full[input_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the correlation between Features and Display on a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr()\n",
    "\n",
    "mask = np.zeros_like(corr_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]= True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 20)) \n",
    "heatmap = sns.heatmap(corr_matrix, \n",
    "                      mask = mask,\n",
    "                      square = True,\n",
    "                      linewidths = .5,\n",
    "                      cmap = 'coolwarm',\n",
    "                      cbar_kws = {'shrink': .4, \n",
    "                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n",
    "                      vmin = -1, \n",
    "                      vmax = 1,\n",
    "                      annot = True,\n",
    "                      annot_kws = {'size': 12})#add the column names as labels\n",
    "ax.set_yticklabels(corr_matrix.columns, rotation = 0)\n",
    "ax.set_xticklabels(corr_matrix.columns)\n",
    "sns.set_style({'xtick.bottom': True}, {'ytick.left': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that loop 12 ('S' type - paired stem) has good correlation with Structure.  Especially Structure 2, i.e. unpaired bases has a -1 correlation, which means they are negatively correlated. This is correct since an unpaired base cannot form a paired stem. And structure 0 and structure 1 has correlation 0.58 each, which is also intuitive as they are the paired bases forming the paired stem. Hence it can be hypothesised that the structure fields do not add any new information over the loop 12 filed and can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check these assumptions with a feature importance metric derived from Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "importances = pd.DataFrame({'feature':input_cols,'importance':np.round(model.feature_importances_,3)})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=importances['importance'], y=importances['feature'])\n",
    "plt.title('Feaure Importance')\n",
    "plt.tight_layout()\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that first four features amount to almost 90% of information. Base unpaired information (structure 2), loop 8(E), loop 12(S), loop 9(H), loop 10(I) and bases can provide almost the full information. Structure 0 and structure 1 contribution is less because this information is there in loop12. But eventhough structure 2 and loop 12 have a -1 correlation, that is not depicted here. This is here the forest is unable to decode this link, Lets probe further to see if we can use that information as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature:Unpaired Probability\n",
    "\n",
    "Lets repeat the above analysis with unpaired probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['unpaired_probability','Base_3','Base_4','Base_5','Base_6','Structure_0','Structure_1','Structure_2','Loop_7','Loop_8','Loop_9','Loop_10','Loop_11','Loop_12','Loop_13']\n",
    "X_train = X_train_full[input_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr()\n",
    "\n",
    "mask = np.zeros_like(corr_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]= True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 20)) \n",
    "heatmap = sns.heatmap(corr_matrix, \n",
    "                      mask = mask,\n",
    "                      square = True,\n",
    "                      linewidths = .5,\n",
    "                      cmap = 'coolwarm',\n",
    "                      cbar_kws = {'shrink': .4, \n",
    "                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n",
    "                      vmin = -1, \n",
    "                      vmax = 1,\n",
    "                      annot = True,\n",
    "                      annot_kws = {'size': 12})#add the column names as labels\n",
    "ax.set_yticklabels(corr_matrix.columns, rotation = 0)\n",
    "ax.set_xticklabels(corr_matrix.columns)\n",
    "sns.set_style({'xtick.bottom': True}, {'ytick.left': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that there is high negative correlation between unpaired probability and loop 12, structure information. This is also intuitive as paired bases will have near to zero unpaired probability and unpaired bases will have near to one unpaired probability. So it seems that this single feature can encode the information in these 4 fields. Lets check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "importances = pd.DataFrame({'feature':input_cols,'importance':np.round(model.feature_importances_,3)})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=importances['importance'], y=importances['feature'])\n",
    "plt.title('Feaure Importance')\n",
    "plt.tight_layout()\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that unpaired probability, base type and loop 8 ('E') have major contribution to output. And as hypothesised, the contributions of structure and loop 12 have gone down which is a good indication. It can be also noted that unpaired probability contributes about 85% of all the information,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dummy submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be overwritten if everything works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/stanford-covid-vaccine/sample_submission.csv', index_col= 0)\n",
    "submission.to_csv('submission.csv')\n",
    "submission.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to make 107 / 130 predictions per input for the 5 parameters (only 3 of these are used for scoring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems you will need something like an LSTM that works on sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform your prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling dummy values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use values mean and std from train data as a dummy submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = submission.shape[0]\n",
    "reactivity_bar = 0.374922; reactivity_sd = 0.725652\n",
    "deg_Mg_pH10_bar = 0.446303; deg_Mg_pH10_sd = 0.704172\n",
    "deg_pH10_bar = 0.446911; deg_pH10_sd = 1.285747\n",
    "deg_Mg_50C_bar = 0.407030; deg_Mg_50C_sd = 0.868013\n",
    "deg_50C_bar = 0.425889; deg_50C_sd = 1.122356\n",
    "submission['reactivity']=np.random.normal(reactivity_bar, reactivity_sd, size=n)\n",
    "submission['deg_Mg_pH10']=np.random.normal(deg_Mg_pH10_bar, deg_Mg_pH10_sd, size=n)\n",
    "submission['deg_pH10']=np.random.normal(deg_pH10_bar, deg_pH10_sd, size=n)\n",
    "submission['deg_Mg_50C']=np.random.normal(deg_Mg_50C_bar, deg_Mg_50C_sd, size=n)\n",
    "submission['deg_50C']=np.random.normal(deg_50C_bar, deg_50C_sd, size=n)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
